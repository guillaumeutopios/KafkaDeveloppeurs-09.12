üóÇÔ∏è Structure des Fichiers

Voici la structure typique du projet :

üì¶ src
 ‚î£ üìÇ main
 ‚îÉ ‚î£ üìÇ java
 ‚îÉ ‚îÉ ‚îó üìÇ com.example.demo_stream_kafka
 ‚îÉ ‚îÉ   ‚î£ üìú DemoStreamKafkaApplication.java  // Point d'entr√©e de l'application Spring Boot
 ‚îÉ ‚îÉ   ‚î£ üìú StreamProcessService.java       // Contient la logique Kafka Streams
 ‚îÉ ‚îÉ   ‚îó üìú KafkaStreamsConfig.java        // Contient la configuration Kafka Streams
 ‚îÉ ‚î£ üìÇ resources
 ‚îÉ ‚îÉ ‚îó üìú application.properties            // Contient la configuration de Kafka (topics, bootstrap, serdes, etc.)

üìò Fichier 1 : application.properties

Le fichier application.properties contient la configuration de l‚Äôapplication Spring Boot et Kafka Streams.

Contenu du fichier :

# Config de Kafka Streams
spring.kafka.streams.application-id=demo-stream-kafka
spring.kafka.streams.bootstrap-servers=kafka-1:19091

# Serde par d√©faut pour les cl√©s et les valeurs (s√©rialisation/d√©s√©rialisation)
spring.kafka.streams.default-key-serde=org.apache.kafka.common.serialization.Serdes$StringSerde
spring.kafka.streams.default-value-serde=org.apache.kafka.common.serialization.Serdes$StringSerde

# Nombre de threads pour Kafka Streams
spring.kafka.streams.threads=1

üîç Explications des propri√©t√©s

	1.	spring.kafka.streams.application-id
‚û°Ô∏è Identifiant unique de l‚Äôapplication Kafka Streams. Chaque application Kafka Streams doit avoir un application-id unique, car Kafka utilise cet ID pour suivre l‚Äô√©tat des t√¢ches de traitement.
	2.	spring.kafka.streams.bootstrap-servers
‚û°Ô∏è Adresse des serveurs Kafka. C‚Äôest ici que Kafka Streams se connecte.
	3.	spring.kafka.streams.default-key-serde et spring.kafka.streams.default-value-serde
‚û°Ô∏è D√©finit le Serde par d√©faut pour les cl√©s et les valeurs. Un Serde est une combinaison de Serializer et de Deserializer. Ici, nous utilisons la s√©rialisation de type String.
	4.	spring.kafka.streams.threads
‚û°Ô∏è Nombre de threads utilis√©s pour ex√©cuter les t√¢ches du flux Kafka Streams. Par d√©faut, c‚Äôest 1.


üìò Fichier 3 : KafkaStreamsConfig.java

Ce fichier configure explicitement les propri√©t√©s de Kafka Streams si vous ne souhaitez pas tout mettre dans application.properties.

Contenu du fichier :

package com.example.demo_stream_kafka;

import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.common.serialization.Serdes;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

import java.util.Properties;

@Configuration
public class KafkaStreamsConfig {

    @Bean(name = "streamConfig")
    public Properties kafkaStreamsProperties() {
        Properties props = new Properties();
        
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "demo-stream-kafka");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka-1:19091");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
        
        return props;
    }
}

üîç Explications

	1.	@Configuration
‚û°Ô∏è Indique √† Spring que cette classe contient des Beans de configuration.
	2.	@Bean(name = ‚ÄústreamConfig‚Äù)
‚û°Ô∏è D√©finit un Bean qui fournit les propri√©t√©s Kafka Streams.
	3.	Properties
‚û°Ô∏è Ce sont les propri√©t√©s de configuration Kafka Streams, qui incluent le APPLICATION_ID_CONFIG, le BOOTSTRAP_SERVERS_CONFIG, etc.

üìò Fichier 4 : StreamProcessService.java

Le fichier le plus important. Ce fichier configure et ex√©cute le flux Kafka Streams.

Contenu du fichier :

package com.example.demo_stream_kafka;

import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.kstream.Consumed;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Produced;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class StreamProcessService {

    @Bean
    public KStream<String, String> kStream(StreamsBuilder streamsBuilder) {
        KStream<String, String> kStream = streamsBuilder.stream(
            "demo-stream-topic", 
            Consumed.with(Serdes.String(), Serdes.String())
        );

        KStream<String, String> processedStream = kStream
            .mapValues(value -> "[" + value.toUpperCase() + "]")
            .peek((key, value) -> System.out.println("üîÑ Transformed message: key=" + key + ", value=" + value));

        processedStream.to("demo-output-stream-topic", 
            Produced.with(Serdes.String(), Serdes.String())
        );

        return processedStream;
    }
}

üîç Explications

	1.	@Configuration
‚û°Ô∏è Indique que cette classe d√©finit des Beans.
	2.	@Bean
‚û°Ô∏è La m√©thode kStream est un Bean Kafka Streams qui retourne un KStream<String, String>.
	3.	KStream<String, String> kStream
‚û°Ô∏è C‚Äôest le flux (stream) d‚Äôentr√©e √† partir du topic demo-stream-topic. Le flux est consomm√© avec Consumed.with(Serdes.String(), Serdes.String()) qui indique que la cl√© et la valeur sont des cha√Ænes de caract√®res.
	4.	kStream.mapValues()
‚û°Ô∏è Transforme les valeurs des messages, ici il met les valeurs en majuscules et ajoute des crochets [ ] autour.
	5.	kStream.to()
‚û°Ô∏è Envoie les messages transform√©s au topic demo-output-stream-topic.

üî• √âtapes de bout en bout : De l‚Äôenvoi au traitement du message

	1.	Cr√©ation des topics

docker exec kafka-1 kafka-topics --create --topic demo-stream-topic --bootstrap-server kafka-1:19091 --partitions 1 --replication-factor 1
docker exec kafka-1 kafka-topics --create --topic demo-output-stream-topic --bootstrap-server kafka-1:19091 --partitions 1 --replication-factor 1


	2.	Envoi d‚Äôun message au topic source

docker exec -it kafka-1 kafka-console-producer --broker-list kafka-1:19091 --topic demo-stream-topic
> hello


	3.	Transformation par Kafka Streams
Le message est transform√© par le flux hello ‚Üí [HELLO].
	4.	Lecture dans le topic de sortie

docker exec -it kafka-1 kafka-console-consumer --bootstrap-server kafka-1:19091 --topic demo-output-stream-topic --from-beginning

Flux des donn√©es :

demo-stream-topic (input) ‚û°Ô∏è traitement (mapValues) ‚û°Ô∏è demo-output-stream-topic (output)


Les autres flux de la d√©mos :

- Cr√©er les topics 

kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-stream-topic

kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-output-stream-topic

kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-stream-filter-topic


kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-output-filter-topic

kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-stream-map-topic


kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-output-map-topic

kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-stream-branch-topic


kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-output-branch-topic-1

kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-output-branch-topic-2

kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-stream-aggregate-topic


kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-output-aggregation-topic

kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-stream-merge1-topic

kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-stream-merge2-topic

kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic demo-output-merge-topic

kafka-topics --create \
    --bootstrap-server kafka-1:19091 \
    --replication-factor 1 \
    --partitions 1 \
    --topic other-demo-stream-topic


curl -X POST 'http://localhost:8080/api/messages/send/demo-stream' \
    -d 'key=user1' \
    -d 'value=hello world'
# Consommer les messages du flux kStream()
kafka-console-consumer --topic demo-output-stream-topic --from-beginning --bootstrap-server kafka-1:19091


curl -X POST 'http://localhost:8080/api/messages/send/demo-output-filter?key=user2&value=important update for the team'
# Consommer les messages du flux filterStream()
kafka-console-consumer --topic demo-output-filter-topic --from-beginning --bootstrap-server kafka-1:19091

curl -X POST 'http://localhost:8080/api/messages/send/demo-output-map?key=user3&value=hello world'
# Consommer les messages du flux mapValuesStream()
kafka-console-consumer --topic demo-output-map-topic --from-beginning --bootstrap-server kafka-1:19091


curl -X POST 'http://localhost:8080/api/messages/send/demo-output-branch-1?key=user4&value=error critical issue'

curl -X POST 'http://localhost:8080/api/messages/send/demo-output-branch-2?key=user4&value=info user has logged in'

# Consommer les messages des branches du flux branchStream()
kafka-console-consumer --topic demo-output-branch-topic-1 --from-beginning --bootstrap-server kafka-1:19091
kafka-console-consumer --topic demo-output-branch-topic-2 --from-beginning --bootstrap-server kafka-1:19091

curl -X POST 'http://localhost:8080/api/messages/send/demo-output-aggregation?key=user6&value=aggregation message'
# Consommer les messages du flux aggregateStream()
kafka-console-consumer --topic demo-output-aggregation-topic --from-beginning --bootstrap-server kafka-1:19091


curl -X POST 'http://localhost:8080/api/messages/send/demo-output-merge?key=user5&value=merge content from two streams'
# Consommer les messages du flux mergeStreams()
kafka-console-consumer --topic demo-output-merge-topic --from-beginning --bootstrap-server kafka-1:19091